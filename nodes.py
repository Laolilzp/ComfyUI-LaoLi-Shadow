import torch
import torch.nn as nn
import comfy.model_management as mm
import comfy.sd
import comfy.controlnet
import comfy.utils
import gc
import os
import sys
import time
import copy
from comfy.model_patcher import ModelPatcher

try:
    import psutil
except ImportError:
    psutil = None

# ==========================================================
# 0. åŸºç¡€å·¥å…· (Utilities)
# ==========================================================
class AnyType(str):
    def __ne__(self, __value: object) -> bool: return False
    def __eq__(self, __value: object) -> bool: return True

any_type = AnyType("*")

# ==========================================================
# 1. å…¨å±€é…ç½® (Global Config)
# ==========================================================
class Shadow_Config:
    enabled = True
    mode = "Ease Mode" 
    shadow_mode = True 
    ram_reserve_gb = 4.0
    verbose = True
    vram_cushion_gb = 1.0 

# ==========================================================
# 2. å½±å­ç³»ç»Ÿ (The Shadow Legion)
# ==========================================================
class ShadowGroup:
    def __init__(self, name, loader_func, *args, **kwargs):
        self.name = name
        self.loader_func = loader_func
        self.args = args
        self.kwargs = kwargs
        self.is_loaded = False
        self.cached_model = None
        self.cached_clip = None
        self.cached_vae = None

    def _execute_load(self):
        if self.is_loaded: return
        if Shadow_Config.verbose: print(f"ğŸ‘» [LaoLi Shadow] è§¦å‘ Checkpoint åŠ è½½: {self.name} ...")
        
        if psutil:
            mem = psutil.virtual_memory()
            available_gb = mem.available / (1024**3)
            if available_gb < Shadow_Config.ram_reserve_gb:
                if Shadow_Config.verbose: print(f"âš ï¸ [LaoLi Shadow] å‰©ä½™å†…å­˜è¿‡ä½ -> è§¦å‘GC")
                gc.collect()

        start_t = time.time()
        out = self.loader_func(*self.args, **self.kwargs)
        self.cached_model = out[0]
        self.cached_clip = out[1]
        self.cached_vae = out[2]
        self.is_loaded = True
        if Shadow_Config.verbose: print(f"âœ¨ [LaoLi Shadow] {self.name} å…¨éƒ¨å°±ç»ª (è€—æ—¶ {time.time()-start_t:.2f}s)")

    def get_real_thing(self, mode):
        if not self.is_loaded: self._execute_load()
        if mode == "model": return self.cached_model
        if mode == "clip": return self.cached_clip
        if mode == "vae": return self.cached_vae
        return None

class ShadowInnerModel(torch.nn.Module):
    def __init__(self, parent_patcher):
        super().__init__()
        self._laoli_parent = parent_patcher 
    def __getattr__(self, name):
        if name.startswith("_laoli") or name.startswith("training"): return super().__getattr__(name)
        if Shadow_Config.verbose:
            if name not in ['device', 'dtype']: print(f"âš¡ [LaoLi Shadow] Deep Access è§¦å‘åŠ è½½: .model.{name}")
        real_patcher = self._laoli_parent._ensure_real()
        return getattr(real_patcher.model, name)

class ShadowPatcher(ModelPatcher):
    def __init__(self, group, *args, **kwargs):
        dummy = ShadowInnerModel(self) 
        super().__init__(dummy, torch.device("cpu"), torch.device("cpu"))
        self._laoli_group = group
        self._laoli_real_obj = None
        self._laoli_is_shadow = True 

    def _ensure_real(self):
        if self._laoli_real_obj is None:
            real = self._laoli_group.get_real_thing("model")
            self.become_real(real)
        return self._laoli_real_obj

    def become_real(self, real_obj):
        if real_obj is None: return 
        self._laoli_real_obj = real_obj
        self.__dict__.update(real_obj.__dict__)
        try: self.__class__ = real_obj.__class__
        except: pass
        if hasattr(self, "_laoli_is_shadow"): del self._laoli_is_shadow
    
    def copy(self): return self.clone()
    def clone(self, *args, **kwargs):
        if self._laoli_real_obj: return self._laoli_real_obj.clone()
        return ShadowPatcher(self._laoli_group)
    def __getattr__(self, name):
        if name.startswith("_laoli"): raise AttributeError(name)
        real = self.__dict__.get("_laoli_real_obj", None)
        if name == "pinned" and real is None: return set()
        if Shadow_Config.verbose and not real:
            if not name.startswith("__"): print(f"âš¡ [LaoLi Shadow] MODEL è§¦å‘åŠ è½½: method '{name}'")
        if real is None: real = self._ensure_real()
        return getattr(real, name)

class ShadowCLIP:
    def __init__(self, group):
        self._laoli_group = group
        self._laoli_real_obj = None
    def _ensure_real(self):
        if self._laoli_real_obj is None:
            real = self._laoli_group.get_real_thing("clip")
            self.become_real(real)
        return self._laoli_real_obj
    def become_real(self, real_obj):
        self._laoli_real_obj = real_obj
        self.__dict__.update(real_obj.__dict__)
        try: self.__class__ = real_obj.__class__
        except: pass
    def clone(self):
        if self._laoli_real_obj: return self._laoli_real_obj.clone()
        return ShadowCLIP(self._laoli_group)
    def copy(self): return self.clone()
    def __getattr__(self, name):
        if name.startswith("_laoli"): raise AttributeError(name)
        if Shadow_Config.verbose and not self._laoli_real_obj:
            print(f"âš¡ [LaoLi Shadow] CLIP è§¦å‘åŠ è½½: method '{name}'")
        real = self._ensure_real()
        return getattr(real, name)

class ShadowVAE:
    def __init__(self, group):
        self._laoli_group = group
        self._laoli_real_obj = None
    def _ensure_real(self):
        if self._laoli_real_obj is None:
            real = self._laoli_group.get_real_thing("vae")
            self.become_real(real)
        return self._laoli_real_obj
    def become_real(self, real_obj):
        self._laoli_real_obj = real_obj
        self.__dict__.update(real_obj.__dict__)
        try: self.__class__ = real_obj.__class__
        except: pass
    def decode(self, samples_in): return self._ensure_real().decode(samples_in)
    def encode(self, pixel_samples): return self._ensure_real().encode(pixel_samples)
    def __getattr__(self, name):
        if name.startswith("_laoli"): raise AttributeError(name)
        if Shadow_Config.verbose and not self._laoli_real_obj:
            print(f"âš¡ [LaoLi Shadow] VAE è§¦å‘åŠ è½½: method '{name}'")
        real = self._ensure_real()
        return getattr(real, name)

# ==========================================================
# 3. åŠ«æŒåŠ è½½å™¨
# ==========================================================
if not hasattr(comfy.sd, "_laoli_org_load_ckpt"):
    comfy.sd._laoli_org_load_ckpt = comfy.sd.load_checkpoint_guess_config

def _hacked_load_checkpoint(ckpt_path, output_vae=True, output_clip=True, embedding_directory=None):
    if Shadow_Config.enabled and Shadow_Config.shadow_mode:
        name = os.path.basename(ckpt_path)
        if Shadow_Config.verbose: print(f"ğŸ’¤ [LaoLi Shadow] æ‹¦æˆª Checkpoint: {name} -> å»ºç«‹å½±å­é˜µåˆ—")
        group = ShadowGroup(name, comfy.sd._laoli_org_load_ckpt, ckpt_path, output_vae=output_vae, output_clip=output_clip, embedding_directory=embedding_directory)
        return (ShadowPatcher(group), ShadowCLIP(group), ShadowVAE(group))
    return comfy.sd._laoli_org_load_ckpt(ckpt_path, output_vae, output_clip, embedding_directory)
comfy.sd.load_checkpoint_guess_config = _hacked_load_checkpoint

class ShadowControlNet(ModelPatcher):
    def __init__(self, name, loader):
        dummy = torch.nn.Module()
        super().__init__(dummy, torch.device("cpu"), torch.device("cpu"))
        self._laoli_is_shadow = True
        self._laoli_name = name
        self._laoli_loader = loader
        self._laoli_real = None
    def summon(self):
        if self._laoli_real: return self._laoli_real
        if Shadow_Config.verbose: print(f"ğŸ‘» [LaoLi Shadow] åŠ è½½ ControlNet: {self._laoli_name}")
        self._laoli_real = self._laoli_loader()
        self.__dict__.update(self._laoli_real.__dict__)
        try: self.__class__ = self._laoli_real.__class__
        except: pass
        if hasattr(self, "_laoli_is_shadow"): del self._laoli_is_shadow
        return self._laoli_real
    def copy(self):
        if self._laoli_real: return self._laoli_real.copy()
        return ShadowControlNet(self._laoli_name, self._laoli_loader)
    def __getattr__(self, name):
        if name.startswith("_laoli"): raise AttributeError(name)
        real = self.__dict__.get("_laoli_real", None)
        if name == "pinned" and real is None: return set()
        self.summon()
        return getattr(self._laoli_real, name)

if not hasattr(comfy.controlnet, "_laoli_org_load_cn"):
    comfy.controlnet._laoli_org_load_cn = comfy.controlnet.load_controlnet
def _hacked_load_controlnet(ckpt_path):
    if Shadow_Config.enabled and Shadow_Config.shadow_mode:
        name = os.path.basename(ckpt_path)
        if Shadow_Config.verbose: print(f"ğŸ’¤ [LaoLi Shadow] æ‹¦æˆª ControlNet: {name} -> å½±å­æ¨¡å¼")
        return ShadowControlNet(name, lambda: comfy.controlnet._laoli_org_load_cn(ckpt_path))
    return comfy.controlnet._laoli_org_load_cn(ckpt_path)
comfy.controlnet.load_controlnet = _hacked_load_controlnet

# ==========================================================
# 4. æ˜¾å­˜ç®¡ç†ä¸è§¦å‘å™¨
# ==========================================================
if not hasattr(mm, "_laoli_original_load_models_gpu"):
    mm._laoli_original_load_models_gpu = mm.load_models_gpu
def _shadow_load_models_gpu(models, memory_required=0, **kwargs):
    if Shadow_Config.enabled:
        try:
            device = mm.get_torch_device()
            for model in models:
                if getattr(model, "_laoli_is_shadow", False):
                    if hasattr(model, "summon"): model.summon() 
                    if hasattr(model, "_ensure_real"): model._ensure_real()
            if Shadow_Config.mode == "Ease Mode":
                all_loaded = True
                for model in models:
                    if hasattr(model, "current_device"):
                        if model.current_device != device: all_loaded = False; break
                    else: all_loaded = False; break
                if not all_loaded:
                    if device.type == 'cuda':
                        stats = torch.cuda.get_device_properties(device)
                        free = stats.total_memory - torch.cuda.memory_reserved(device)
                        needed = memory_required if memory_required > 0 else (1.0 * 1024**3)
                        cushion = Shadow_Config.vram_cushion_gb * 1024**3
                        if free < (needed + cushion):
                            if Shadow_Config.verbose: print(f"ğŸ§¹ [LaoLi Shadow] æ˜¾å­˜ä¸è¶³ (ä½™{free/1024**3:.1f}G) -> æ¸…ç†")
                            mm.unload_all_models()
                            mm.soft_empty_cache()
                            if device.type == 'cuda': torch.cuda.empty_cache()
            if psutil:
                mem = psutil.virtual_memory()
                available_gb = mem.available / (1024**3)
                if available_gb < Shadow_Config.ram_reserve_gb:
                     if Shadow_Config.verbose: print(f"âš ï¸ [LaoLi Shadow] å‰©ä½™å†…å­˜è¿‡ä½ -> GC")
                     gc.collect()
        except Exception as e: print(f"âŒ [LaoLi Shadow Error] {e}")
    return mm._laoli_original_load_models_gpu(models, memory_required=memory_required, **kwargs)
mm.load_models_gpu = _shadow_load_models_gpu

# ==========================================================
# 5. èŠ‚ç‚¹å®šä¹‰ (Nodes Definition)
# ==========================================================

class LaoLi_Shadow_Node:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "enable": ("BOOLEAN", {"default": True}),
                "shadow_mode": ("BOOLEAN", {"default": True}),
                "mode": (["Ease Mode", "Monitor Mode"],),
                "ram_reserve_gb": ("FLOAT", {"default": 4.0, "min": 0.5, "max": 64.0, "step": 0.5}),
                "verbose": ("BOOLEAN", {"default": True}),
            }
        }
    RETURN_TYPES = ()
    FUNCTION = "update_settings"
    CATEGORY = "LaoLi Shadow"
    DESCRIPTION = "ğŸ‘» è€æ_å½±å­ (Shadow)  "
    def update_settings(self, enable, shadow_mode, mode, ram_reserve_gb, verbose):
        Shadow_Config.enabled = enable
        Shadow_Config.shadow_mode = shadow_mode
        Shadow_Config.mode = mode
        Shadow_Config.ram_reserve_gb = float(ram_reserve_gb)
        Shadow_Config.verbose = verbose
        status = "âœ… å¼€å¯" if enable else "â¸ï¸ æš‚åœ"
        print(f"\nğŸ‘» [LaoLi Shadow] {status} | æ¨¡å¼: {shadow_mode} | å†…å­˜ä¿ç•™: {ram_reserve_gb}GB")
        return ()

class LaoLi_Flow_Gate:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": { "input_data": (any_type, {"tooltip": "è¿æ¥ä»»ä½•æ•°æ®"}) },
            "optional": { "wait_for": (any_type, {"tooltip": "è¿æ¥å…ˆå†³æ¡ä»¶"}) }
        }
    RETURN_TYPES = (any_type,) 
    FUNCTION = "run"
    CATEGORY = "LaoLi Shadow"
    DESCRIPTION = "å¼ºè¡Œè®© ComfyUI ç­‰å¾… 'wait_for' å®Œæˆåï¼Œæ‰é‡Šæ”¾ 'input_data'ã€‚"
    def run(self, input_data, wait_for=None): return (input_data,)

class LaoLi_Lineup_Node:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": (any_type,),
                "vram_threshold": ("FLOAT", {"default": 0.85, "min": 0.1, "max": 1.0, "step": 0.05}),
                "cleaning_interval": ("INT", {"default": 1, "min": 1, "max": 10, "step": 1}),
                "strict_mode": ("BOOLEAN", {"default": True}),
            }
        }
    RETURN_TYPES = (any_type,)
    RETURN_NAMES = ("optimized_model",)
    FUNCTION = "apply_lineup"
    CATEGORY = "LaoLi Shadow" 
    DESCRIPTION = "è€æ Lineup : å½±å­ç©¿é€+å…¨å±€ç®—æ³•ã€‚"

    def apply_lineup(self, model, vram_threshold, cleaning_interval, strict_mode):
        target_model_wrapper = model
        try:
            # 1. å¦‚æœæ˜¯å½±å­ï¼Œå¿…é¡»å¼ºåˆ¶ç°èº«ï¼Œå¦åˆ™æ‰«æä¸åˆ°ä»»ä½•å±‚
            if getattr(model, "_laoli_is_shadow", False):
                if Shadow_Config.verbose: print(f"âš¡ [LaoLi Lineup] æ£€æµ‹åˆ°å½±å­ï¼Œæ­£åœ¨å¼ºåˆ¶åŠ è½½çœŸèº«ä»¥è¿›è¡Œä¼˜åŒ–...")
                model._ensure_real() # å¼ºåˆ¶è¯»ç›˜
                target_model_wrapper = model._laoli_real_obj # æ‹¿åˆ°çœŸèº«(ModelPatcher)
            elif hasattr(model, "clone"):
                try: target_model_wrapper = model.clone()
                except: target_model_wrapper = model
            
            # 2. å‡†å¤‡é’©å­
            device = mm.get_torch_device()
            total_vram = 0
            if device.type == 'cuda':
                total_vram = torch.cuda.get_device_properties(device).total_memory
            
            def smart_hook(module, input):
                if total_vram == 0: return None
                current_reserved = torch.cuda.memory_reserved(device)
                usage_ratio = current_reserved / total_vram
                if usage_ratio >= vram_threshold:
                    if strict_mode: torch.cuda.synchronize() 
                    mm.soft_empty_cache()       
                return None

            # 3. å…¨å±€ç®—æ³• (æ‰«æçœŸèº«)
            best_container = self._find_dominant_layer_container(target_model_wrapper)

            if best_container is None:
                if Shadow_Config.verbose: 
                    print(f"âš ï¸ [LaoLi Lineup] æ‰«æå¤±è´¥: {type(target_model_wrapper).__name__} å†…éƒ¨æ²¡æœ‰å‘ç°ä»»ä½•å±‚åˆ—è¡¨ã€‚")
                # å³ä½¿å¤±è´¥ä¹Ÿè¿”å›çœŸèº«(æˆ–å…‹éš†ä½“)ï¼Œä¸è¦è¿”å›å½±å­ï¼Œå¦åˆ™é‡‡æ ·å™¨å¯èƒ½ä¸è®¤
                return (target_model_wrapper,)

            blocks = list(best_container)
            mounted_count = 0
            
            for i, block in enumerate(blocks):
                if i % cleaning_interval == 0:
                    block.register_forward_pre_hook(smart_hook)
                    mounted_count += 1

            if Shadow_Config.verbose:
                print(f"ğŸš€ [LaoLi Lineup] æ³¨å…¥æˆåŠŸ | ç›®æ ‡: {len(blocks)}å±‚ç»“æ„ | æŒ‚è½½: {mounted_count}å±‚ | é˜ˆå€¼: {int(vram_threshold*100)}%")
            
            return (target_model_wrapper,)

        except Exception as e:
            print(f"âŒ [LaoLi Lineup Error] {e}")
            return (model,)

    def _find_dominant_layer_container(self, root_obj):
        best_container = None
        max_len = 0
        
        # å®šä¹‰æœç´¢ç”Ÿæˆå™¨ï¼Œè‡ªåŠ¨å¤„ç† ModelPatcher å’Œ Module
        def iter_modules(obj):
            if isinstance(obj, ModelPatcher):
                # ç¡®ä¿è¿™é‡Œæ‹¿åˆ°çš„æ˜¯çœŸçš„ model
                yield from obj.model.named_modules()
            elif isinstance(obj, torch.nn.Module):
                yield from obj.named_modules()
            else:
                # é¸­å­ç±»å‹å°è¯•
                if hasattr(obj, "model") and isinstance(obj.model, torch.nn.Module):
                    yield from obj.model.named_modules()

        for name, module in iter_modules(root_obj):
            if isinstance(module, (nn.ModuleList, nn.Sequential)):
                curr_len = len(module)
                # Qwen/Wan çš„å±‚æ•°é€šå¸¸ > 4
                if curr_len > 4: 
                    if curr_len > max_len:
                        max_len = curr_len
                        best_container = module
        
        return best_container

# --- æ³¨å†ŒèŠ‚ç‚¹ ---
NODE_CLASS_MAPPINGS = {
    "LaoLi_Shadow": LaoLi_Shadow_Node,
    "LaoLi_Flow_Gate": LaoLi_Flow_Gate,
    "LaoLi_Lineup": LaoLi_Lineup_Node
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "LaoLi_Shadow": "ğŸ‘» è€æ_å½±å­ (Shadow)",
    "LaoLi_Flow_Gate": "ğŸš§ è€æ_é€»è¾‘é—¨ (Flow Gate)",
    "LaoLi_Lineup": "ğŸš€ è€æ_æ’é˜Ÿ (Lineup VRAM)"
}