import torch
import torch.nn as nn
import comfy.model_management as mm
import comfy.sd
import comfy.controlnet
import comfy.utils
import gc
import os
import sys
import time
import copy
from comfy.model_patcher import ModelPatcher

try:
    import psutil
except ImportError:
    psutil = None

# ==========================================================
# 0. åŸºç¡€å·¥å…· (Utilities)
# ==========================================================
class AnyType(str):
    def __ne__(self, __value: object) -> bool: return False
    def __eq__(self, __value: object) -> bool: return True

any_type = AnyType("*")

# ==========================================================
# 1. å…¨å±€é…ç½® (Global Config)
# ==========================================================
class Shadow_Config:
    enabled = True
    mode = "Ease Mode" 
    shadow_mode = True 
    ram_reserve_gb = 4.0  # å†…å­˜ä¿ç•™é»˜è®¤å€¼
    vram_reserve_mb = 1024.0  # æ˜¾å­˜ä¿ç•™é»˜è®¤å€¼
    verbose = True
    vram_cushion_gb = 1.0 

# ==========================================================
# 2. å½±å­ç³»ç»Ÿ (The Shadow Legion)
# ==========================================================
class ShadowGroup:
    def __init__(self, name, loader_func, *args, **kwargs):
        self.name = name
        self.loader_func = loader_func
        self.args = args
        self.kwargs = kwargs
        self.is_loaded = False
        self.cached_model = None
        self.cached_clip = None
        self.cached_vae = None

    def _execute_load(self):
        if self.is_loaded: return
        if Shadow_Config.verbose: print(f"ğŸ‘» [LaoLi Shadow] è§¦å‘ Checkpoint åŠ è½½: {self.name} ...")
        
        if psutil:
            mem = psutil.virtual_memory()
            available_gb = mem.available / (1024**3)
            if available_gb < Shadow_Config.ram_reserve_gb:
                if Shadow_Config.verbose: print(f"âš ï¸ [LaoLi Shadow] å‰©ä½™å†…å­˜è¿‡ä½ -> è§¦å‘GC")
                gc.collect()

        start_t = time.time()
        out = self.loader_func(*self.args, **self.kwargs)
        self.cached_model = out[0]
        self.cached_clip = out[1]
        self.cached_vae = out[2]
        self.is_loaded = True
        if Shadow_Config.verbose: print(f"âœ¨ [LaoLi Shadow] {self.name} å…¨éƒ¨å°±ç»ª (è€—æ—¶ {time.time()-start_t:.2f}s)")

    def get_real_thing(self, mode):
        if not self.is_loaded: self._execute_load()
        if mode == "model": return self.cached_model
        if mode == "clip": return self.cached_clip
        if mode == "vae": return self.cached_vae
        return None

class ShadowInnerModel(torch.nn.Module):
    def __init__(self, parent_patcher):
        super().__init__()
        self._laoli_parent = parent_patcher 
    def __getattr__(self, name):
        if name.startswith("_laoli") or name.startswith("training"): return super().__getattr__(name)
        if Shadow_Config.verbose:
            if name not in ['device', 'dtype']: print(f"âš¡ [LaoLi Shadow] Deep Access è§¦å‘åŠ è½½: .model.{name}")
        real_patcher = self._laoli_parent._ensure_real()
        return getattr(real_patcher.model, name)

class ShadowPatcher(ModelPatcher):
    def __init__(self, group, *args, **kwargs):
        dummy = ShadowInnerModel(self) 
        super().__init__(dummy, torch.device("cpu"), torch.device("cpu"))
        self._laoli_group = group
        self._laoli_real_obj = None
        self._laoli_is_shadow = True 

    def _ensure_real(self):
        if self._laoli_real_obj is None:
            real = self._laoli_group.get_real_thing("model")
            self.become_real(real)
        return self._laoli_real_obj

    def become_real(self, real_obj):
        if real_obj is None: return 
        self._laoli_real_obj = real_obj
        self.__dict__.update(real_obj.__dict__)
        try: self.__class__ = real_obj.__class__
        except: pass
        if hasattr(self, "_laoli_is_shadow"): del self._laoli_is_shadow
    
    def copy(self): return self.clone()
    def clone(self, *args, **kwargs):
        if self._laoli_real_obj: return self._laoli_real_obj.clone()
        return ShadowPatcher(self._laoli_group)
    def __getattr__(self, name):
        if name.startswith("_laoli"): raise AttributeError(name)
        real = self.__dict__.get("_laoli_real_obj", None)
        if name == "pinned" and real is None: return set()
        if Shadow_Config.verbose and not real:
            if not name.startswith("__"): print(f"âš¡ [LaoLi Shadow] MODEL è§¦å‘åŠ è½½: method '{name}'")
        if real is None: real = self._ensure_real()
        return getattr(real, name)

class ShadowCLIP:
    def __init__(self, group):
        self._laoli_group = group
        self._laoli_real_obj = None
    def _ensure_real(self):
        if self._laoli_real_obj is None:
            real = self._laoli_group.get_real_thing("clip")
            self.become_real(real)
        return self._laoli_real_obj
    def become_real(self, real_obj):
        self._laoli_real_obj = real_obj
        self.__dict__.update(real_obj.__dict__)
        try: self.__class__ = real_obj.__class__
        except: pass
    def clone(self):
        if self._laoli_real_obj: return self._laoli_real_obj.clone()
        return ShadowCLIP(self._laoli_group)
    def copy(self): return self.clone()
    def __getattr__(self, name):
        if name.startswith("_laoli"): raise AttributeError(name)
        if Shadow_Config.verbose and not self._laoli_real_obj:
            print(f"âš¡ [LaoLi Shadow] CLIP è§¦å‘åŠ è½½: method '{name}'")
        real = self._ensure_real()
        return getattr(real, name)

class ShadowVAE:
    def __init__(self, group):
        self._laoli_group = group
        self._laoli_real_obj = None
    def _ensure_real(self):
        if self._laoli_real_obj is None:
            real = self._laoli_group.get_real_thing("vae")
            self.become_real(real)
        return self._laoli_real_obj
    def become_real(self, real_obj):
        self._laoli_real_obj = real_obj
        self.__dict__.update(real_obj.__dict__)
        try: self.__class__ = real_obj.__class__
        except: pass
    def decode(self, samples_in): return self._ensure_real().decode(samples_in)
    def encode(self, pixel_samples): return self._ensure_real().encode(pixel_samples)
    def __getattr__(self, name):
        if name.startswith("_laoli"): raise AttributeError(name)
        if Shadow_Config.verbose and not self._laoli_real_obj:
            print(f"âš¡ [LaoLi Shadow] VAE è§¦å‘åŠ è½½: method '{name}'")
        real = self._ensure_real()
        return getattr(real, name)

# ==========================================================
# 3. åŠ«æŒåŠ è½½å™¨
# ==========================================================
if not hasattr(comfy.sd, "_laoli_org_load_ckpt"):
    comfy.sd._laoli_org_load_ckpt = comfy.sd.load_checkpoint_guess_config

def _hacked_load_checkpoint(ckpt_path, output_vae=True, output_clip=True, embedding_directory=None):
    if Shadow_Config.enabled and Shadow_Config.shadow_mode:
        name = os.path.basename(ckpt_path)
        if Shadow_Config.verbose: print(f"ğŸ’¤ [LaoLi Shadow] æ‹¦æˆª Checkpoint: {name} -> å»ºç«‹å½±å­é˜µåˆ—")
        group = ShadowGroup(name, comfy.sd._laoli_org_load_ckpt, ckpt_path, output_vae=output_vae, output_clip=output_clip, embedding_directory=embedding_directory)
        return (ShadowPatcher(group), ShadowCLIP(group), ShadowVAE(group))
    return comfy.sd._laoli_org_load_ckpt(ckpt_path, output_vae, output_clip, embedding_directory)
comfy.sd.load_checkpoint_guess_config = _hacked_load_checkpoint

class ShadowControlNet(ModelPatcher):
    def __init__(self, name, loader):
        dummy = torch.nn.Module()
        super().__init__(dummy, torch.device("cpu"), torch.device("cpu"))
        self._laoli_is_shadow = True
        self._laoli_name = name
        self._laoli_loader = loader
        self._laoli_real = None
    def summon(self):
        if self._laoli_real: return self._laoli_real
        if Shadow_Config.verbose: print(f"ğŸ‘» [LaoLi Shadow] åŠ è½½ ControlNet: {self._laoli_name}")
        self._laoli_real = self._laoli_loader()
        self.__dict__.update(self._laoli_real.__dict__)
        try: self.__class__ = self._laoli_real.__class__
        except: pass
        if hasattr(self, "_laoli_is_shadow"): del self._laoli_is_shadow
        return self._laoli_real
    def copy(self):
        if self._laoli_real: return self._laoli_real.copy()
        return ShadowControlNet(self._laoli_name, self._laoli_loader)
    def __getattr__(self, name):
        if name.startswith("_laoli"): raise AttributeError(name)
        real = self.__dict__.get("_laoli_real", None)
        if name == "pinned" and real is None: return set()
        self.summon()
        return getattr(self._laoli_real, name)

if not hasattr(comfy.controlnet, "_laoli_org_load_cn"):
    comfy.controlnet._laoli_org_load_cn = comfy.controlnet.load_controlnet
def _hacked_load_controlnet(ckpt_path):
    if Shadow_Config.enabled and Shadow_Config.shadow_mode:
        name = os.path.basename(ckpt_path)
        if Shadow_Config.verbose: print(f"ğŸ’¤ [LaoLi Shadow] æ‹¦æˆª ControlNet: {name} -> å½±å­æ¨¡å¼")
        return ShadowControlNet(name, lambda: comfy.controlnet._laoli_org_load_cn(ckpt_path))
    return comfy.controlnet._laoli_org_load_cn(ckpt_path)
comfy.controlnet.load_controlnet = _hacked_load_controlnet

# ==========================================================
# 4. æ˜¾å­˜ç®¡ç†ä¸è§¦å‘å™¨ (Safe VRAM Check)
# ==========================================================
if not hasattr(mm, "_laoli_original_load_models_gpu"):
    mm._laoli_original_load_models_gpu = mm.load_models_gpu

def _shadow_load_models_gpu(models, memory_required=0, **kwargs):
    if Shadow_Config.enabled:
        try:
            device = mm.get_torch_device()
            # 1. å”¤é†’å½±å­
            for model in models:
                if getattr(model, "_laoli_is_shadow", False):
                    if hasattr(model, "summon"): model.summon() 
                    if hasattr(model, "_ensure_real"): model._ensure_real()
            
            # 2. Ease Mode æ˜¾å­˜é¢„æ£€æŸ¥
            if Shadow_Config.mode == "Ease Mode":
                all_loaded = True
                for model in models:
                    if hasattr(model, "current_device"):
                        if model.current_device != device: all_loaded = False; break
                    else: all_loaded = False; break
                
                if not all_loaded and device.type == 'cuda':
                    try:
                        free_mem, total_mem = torch.cuda.mem_get_info(device)
                    except:
                        stats = torch.cuda.get_device_properties(device)
                        free_mem = stats.total_memory - torch.cuda.memory_reserved(device)
                    
                    needed = memory_required if memory_required > 0 else (1.0 * 1024**3)
                    
                    # ä½¿ç”¨ Shadow èŠ‚ç‚¹çš„å…¨å±€é¢„ç•™è®¾ç½®
                    reserve_bytes = Shadow_Config.vram_reserve_mb * 1024 * 1024
                    cushion_bytes = Shadow_Config.vram_cushion_gb * 1024**3
                    safe_cushion = max(cushion_bytes, reserve_bytes)

                    if free_mem < (needed + safe_cushion):
                        if Shadow_Config.verbose: 
                            print(f"ğŸ§¹ [LaoLi Shadow] æ˜¾å­˜ä¸è¶³ (çœŸå®å‰©ä½™{free_mem/1024**3:.1f}G | éœ€ä¿ç•™{Shadow_Config.vram_reserve_mb}MB) -> å¼ºåˆ¶æ¸…ç†")
                        mm.unload_all_models()
                        mm.soft_empty_cache()
                        if device.type == 'cuda': torch.cuda.empty_cache()

            # 3. å†…å­˜(RAM) æ£€æŸ¥
            if psutil:
                mem = psutil.virtual_memory()
                available_gb = mem.available / (1024**3)
                if available_gb < Shadow_Config.ram_reserve_gb:
                     if Shadow_Config.verbose: print(f"âš ï¸ [LaoLi Shadow] ç³»ç»Ÿå†…å­˜ä¸è¶³ -> è§¦å‘GC")
                     gc.collect()

        except Exception as e: print(f"âŒ [LaoLi Shadow Error] {e}")
    
    return mm._laoli_original_load_models_gpu(models, memory_required=memory_required, **kwargs)
mm.load_models_gpu = _shadow_load_models_gpu

# ==========================================================
# 5. èŠ‚ç‚¹å®šä¹‰ (Nodes Definition)
# ==========================================================

class LaoLi_Shadow_Node:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "enable": ("BOOLEAN", {"default": True}),
                "shadow_mode": ("BOOLEAN", {"default": True}),
                "mode": (["Ease Mode", "Monitor Mode"],),
                "ram_reserve_gb": ("FLOAT", {"default": 4.0, "min": 0.5, "max": 64.0, "step": 0.5}),
                "vram_reserve_mb": ("FLOAT", {"default": 512.0, "min": 0.0, "max": 8192.0, "step": 64.0, "tooltip": "ä¸ºç³»ç»Ÿé¢„ç•™çš„æ˜¾å­˜(MB)ï¼Œé˜²æ­¢å¡æ­»"}),
                "verbose": ("BOOLEAN", {"default": True}),
            }
        }
    RETURN_TYPES = ()
    FUNCTION = "update_settings"
    CATEGORY = "LaoLi Shadow"
    DESCRIPTION = "ğŸ‘» è€æ_å½±å­ (Shadow) : å…¨å±€æ§åˆ¶ä¸èµ„æºç®¡ç†"
    
    def update_settings(self, enable, shadow_mode, mode, ram_reserve_gb, vram_reserve_mb, verbose):
        Shadow_Config.enabled = enable
        Shadow_Config.shadow_mode = shadow_mode
        Shadow_Config.mode = mode
        Shadow_Config.ram_reserve_gb = float(ram_reserve_gb)
        Shadow_Config.vram_reserve_mb = float(vram_reserve_mb)
        Shadow_Config.verbose = verbose
        status = "âœ… å¼€å¯" if enable else "â¸ï¸ æš‚åœ"
        print(f"\nğŸ‘» [LaoLi Shadow] {status} | æ¨¡å¼: {mode} | VRAMé¢„ç•™: {vram_reserve_mb}MB")
        return ()

class LaoLi_Flow_Gate:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": { "input_data": (any_type, {"tooltip": "è¿æ¥ä»»ä½•æ•°æ®"}) },
            "optional": { "wait_for": (any_type, {"tooltip": "è¿æ¥å…ˆå†³æ¡ä»¶"}) }
        }
    RETURN_TYPES = (any_type,) 
    FUNCTION = "run"
    CATEGORY = "LaoLi Shadow"
    DESCRIPTION = "å¼ºè¡Œè®© ComfyUI ç­‰å¾… 'wait_for' å®Œæˆåï¼Œæ‰é‡Šæ”¾ 'input_data'ã€‚"
    def run(self, input_data, wait_for=None): return (input_data,)

class LaoLi_Lineup_Node:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": (any_type,),
                "vram_threshold": ("FLOAT", {"default": 0.85, "min": 0.1, "max": 1.0, "step": 0.05}),
                "cleaning_interval": ("INT", {"default": 1, "min": 1, "max": 10, "step": 1}),
                "strict_mode": ("BOOLEAN", {"default": True}),
            }
        }
    RETURN_TYPES = (any_type,)
    RETURN_NAMES = ("optimized_model",)
    FUNCTION = "apply_lineup"
    CATEGORY = "LaoLi Shadow" 
    DESCRIPTION = "è€æ Lineup : æ˜¾å­˜æ’é˜Ÿä¸æ·±åº¦ä¼˜åŒ–"

    def apply_lineup(self, model, vram_threshold, cleaning_interval, strict_mode):
        target_model_wrapper = model
        try:
            # 1. å½±å­å¤„ç†
            if getattr(model, "_laoli_is_shadow", False):
                if Shadow_Config.verbose: print(f"âš¡ [LaoLi Lineup] æ£€æµ‹åˆ°å½±å­ï¼Œå¼ºåˆ¶åŠ è½½çœŸèº«...")
                model._ensure_real()
                target_model_wrapper = model._laoli_real_obj
            elif hasattr(model, "clone"):
                try: target_model_wrapper = model.clone()
                except: target_model_wrapper = model
            
            # 2. æ˜¾å­˜è®¡ç®— (åŒé‡é™åˆ¶)
            device = mm.get_torch_device()
            total_vram = 0
            if device.type == 'cuda':
                total_vram = torch.cuda.get_device_properties(device).total_memory
            
            reserve_bytes = Shadow_Config.vram_reserve_mb * 1024 * 1024
            limit_by_ratio = total_vram * vram_threshold
            limit_by_reserve = total_vram - reserve_bytes
            effective_limit_bytes = min(limit_by_ratio, limit_by_reserve)
            
            if Shadow_Config.verbose and total_vram > 0:
                print(f"ğŸ›¡ï¸ [LaoLi Lineup] æ˜¾å­˜å®‰å…¨çº¿: {effective_limit_bytes/1024**3:.2f} GB (å…¨å±€é¢„ç•™: {Shadow_Config.vram_reserve_mb}MB)")

            def smart_hook(module, input):
                if total_vram == 0: return None
                current_reserved = torch.cuda.memory_reserved(device)
                if current_reserved >= effective_limit_bytes:
                    if strict_mode: torch.cuda.synchronize() 
                    mm.soft_empty_cache()       
                return None

            # 3. æœç´¢æ ¸å¿ƒå±‚ (ä¼˜åŒ–ç‰ˆ)
            best_container = self._find_dominant_layer_container(target_model_wrapper)

            if best_container is None:
                if Shadow_Config.verbose: print(f"âš ï¸ [LaoLi Lineup] æ‰«æå®Œæˆï¼Œæœªå‘ç°å¯ç”¨çš„å±‚ç»“æ„ (å¯èƒ½æ¨¡å‹å·²è¢«é«˜åº¦å°è£…)")
                return (target_model_wrapper,)

            blocks = list(best_container)
            mounted_count = 0
            
            for i, block in enumerate(blocks):
                if i % cleaning_interval == 0:
                    block.register_forward_pre_hook(smart_hook)
                    mounted_count += 1
            
            if Shadow_Config.verbose:
                 print(f"ğŸš€ [LaoLi Lineup] æ³¨å…¥æˆåŠŸ | å‘ç°: {len(blocks)}å±‚ | æŒ‚è½½: {mounted_count}ä¸ªé’©å­")

            return (target_model_wrapper,)

        except Exception as e:
            print(f"âŒ [LaoLi Lineup Error] {e}")
            return (model,)

    def _find_dominant_layer_container(self, root_obj):
        # ä¸»åŠ¨å‰¥æ´‹è‘±é€»è¾‘ï¼šä¸“é—¨å¤„ç† ComfyUI çš„ ModelPatcher å’Œ SD/Flux ç»“æ„
        real_model = root_obj
        
        # 1. å‰¥ç¦» ModelPatcher
        if hasattr(real_model, "model"): 
            real_model = real_model.model
            
        # 2. å‰¥ç¦» ComfyUI çš„ BaseModel åŒ…è£… (é’ˆå¯¹ SD/Flux)
        # å¤§å¤šæ•°æ¨¡å‹çš„çœŸæ­£å±‚ç»“æ„åœ¨ diffusion_model å±æ€§ä¸‹
        if hasattr(real_model, "diffusion_model"):
            real_model = real_model.diffusion_model
            
        best_container = None
        max_len = 0
        
        # 3. ç›´æ¥åœ¨å‰¥ç¦»åçš„æ¨¡å‹ä¸­æœç´¢
        try:
            # ä½¿ç”¨ named_modules æœç´¢æ‰€æœ‰å­æ¨¡å—
            for name, module in real_model.named_modules():
                if isinstance(module, (nn.ModuleList, nn.Sequential)):
                    curr_len = len(module)
                    # åªæœ‰é•¿åº¦å¤§äº 4 çš„æ‰è¢«è®¤ä¸ºæ˜¯æ ¸å¿ƒè®¡ç®—å±‚ (æ’é™¤ä¸€äº›å°çš„ embedding list)
                    if curr_len > 4: 
                        if curr_len > max_len:
                            max_len = curr_len
                            best_container = module
        except Exception:
            pass
            
        return best_container

# --- æ³¨å†ŒèŠ‚ç‚¹ ---
NODE_CLASS_MAPPINGS = {
    "LaoLi_Shadow": LaoLi_Shadow_Node,
    "LaoLi_Flow_Gate": LaoLi_Flow_Gate,
    "LaoLi_Lineup": LaoLi_Lineup_Node
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "LaoLi_Shadow": "ğŸ‘» è€æ_å½±å­ (Shadow)",
    "LaoLi_Flow_Gate": "ğŸš§ è€æ_é€»è¾‘é—¨ (Flow Gate)",
    "LaoLi_Lineup": "ğŸš€ è€æ_æ’é˜Ÿ (Lineup VRAM)"
}