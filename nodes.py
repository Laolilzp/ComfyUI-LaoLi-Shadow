import torch
import torch.nn as nn
import comfy.model_management as mm
import comfy.sd
import comfy.controlnet
import comfy.utils
import gc
import os
import sys
import time
import copy
from comfy.model_patcher import ModelPatcher

# å®‰å…¨å¯¼å…¥ psutil
try:
    import psutil
except ImportError:
    psutil = None

# ==========================================================
# 0. åŸºç¡€å·¥å…· (Utilities)
# ==========================================================
class AnyType(str):
    def __ne__(self, __value: object) -> bool: return False
    def __eq__(self, __value: object) -> bool: return True

any_type = AnyType("*")

# ==========================================================
# 1. å…¨å±€é…ç½® (Global Config)
# ==========================================================
class Shadow_Config:
    enabled = False 
    mode = "Ease Mode" 
    job_type = "Image (Aggressive)"
    shadow_mode = True 
    ram_reserve_gb = 4.0
    vram_reserve_mb = 1024.0
    verbose = True

# ==========================================================
# 2. å½±å­ç³»ç»Ÿæ ¸å¿ƒ (The Shadow Legion)
# ==========================================================
class ShadowGroup:
    """ç®¡ç†çœŸå®çš„åŠ è½½è¿‡ç¨‹ï¼Œåªæœ‰åœ¨éœ€è¦æ—¶æ‰è§¦å‘"""
    def __init__(self, name, loader_func, *args, **kwargs):
        self.name = name
        self.loader_func = loader_func
        self.args = args
        self.kwargs = kwargs
        self.is_loaded = False
        self.cached_tuple = None

    def _execute_load(self):
        if self.is_loaded: return
        if Shadow_Config.verbose: print(f"ğŸ‘» [LaoLi Shadow] âš¡ è§¦å‘å»¶è¿ŸåŠ è½½: {self.name}")
        
        # å†…å­˜å®‰å…¨æ£€æŸ¥
        if psutil:
            try:
                mem = psutil.virtual_memory()
                if (mem.available / (1024**3)) < Shadow_Config.ram_reserve_gb:
                    if Shadow_Config.verbose: print(f"âš ï¸ [LaoLi Shadow] ç³»ç»Ÿå†…å­˜åƒç´§ -> è§¦å‘GC")
                    gc.collect()
            except: pass

        start_t = time.time()
        # æ‰§è¡ŒåŸå§‹åŠ è½½
        self.cached_tuple = self.loader_func(*self.args, **self.kwargs)
        self.is_loaded = True
        if Shadow_Config.verbose: print(f"âœ¨ [LaoLi Shadow] {self.name} åŠ è½½å®Œæ¯• (è€—æ—¶ {time.time()-start_t:.2f}s)")

    def get_real_thing(self, index):
        if not self.is_loaded: self._execute_load()
        if self.cached_tuple and isinstance(self.cached_tuple, (list, tuple)) and len(self.cached_tuple) > index:
            return self.cached_tuple[index]
        return None

class ShadowInnerModel(torch.nn.Module):
    """ä»£ç†å†…éƒ¨æ¨¡å‹ï¼Œé˜²æ­¢ ModelPatcher åˆå§‹åŒ–æŠ¥é”™"""
    def __init__(self, parent_patcher):
        super().__init__()
        self._laoli_parent = parent_patcher 
    
    def __getattr__(self, name):
        # æ‹¦æˆªå†…éƒ¨å±æ€§è®¿é—®ï¼Œé˜²æ­¢æ— é™é€’å½’
        if name.startswith("_laoli") or name.startswith("training") or name.startswith("__"): 
             raise AttributeError(f"ShadowInnerModel missing: {name}")
        
        # å”¤é†’çœŸèº«å¹¶è·å–å±æ€§
        real_patcher = self._laoli_parent._ensure_real()
        return getattr(real_patcher.model, name)

class ShadowPatcher(ModelPatcher):
    """ä¼ªè£…æˆ ModelPatcherï¼Œåœ¨æ­¤æœŸé—´æ•è·æ‰€æœ‰ LoRA å’Œè®¾ç½®"""
    def __init__(self, group, *args, **kwargs):
        dummy = ShadowInnerModel(self)
        # é€ä¼ å‚æ•°ï¼Œå…¼å®¹ ComfyUI æ–°ç‰ˆæœ¬
        super().__init__(dummy, torch.device("cpu"), torch.device("cpu"), *args, **kwargs)
        self._laoli_group = group
        self._laoli_real_obj = None
        self._laoli_is_shadow = True 

    def _ensure_real(self):
        if self._laoli_real_obj is None:
            # å‡å®š Checkpoint è¿”å›çš„ç¬¬ä¸€ä¸ªæ˜¯ Model
            real = self._laoli_group.get_real_thing(0)
            self.become_real(real)
        return self._laoli_real_obj

    def become_real(self, real_obj):
        if real_obj is None: return 
        
        # 1. å¤‡ä»½å½±å­æœŸé—´ç§¯ç´¯çš„è¡¥ä¸(LoRA)å’Œå‚æ•°
        preserved_patches = copy.deepcopy(getattr(self, "patches", {}))
        preserved_obj_patches = copy.deepcopy(getattr(self, "object_patches", {}))
        preserved_options = copy.deepcopy(getattr(self, "model_options", {}))
        
        self._laoli_real_obj = real_obj
        
        # 2. æš´åŠ›è¦†ç›–å±æ€§ï¼Œå˜èº«ä¸ºçœŸèº«
        self.__dict__.update(real_obj.__dict__)
        
        # 3. æ¢å¤è¡¥ä¸
        if preserved_patches: self.patches = preserved_patches
        if preserved_obj_patches: self.object_patches = preserved_obj_patches
        if preserved_options:
            current = getattr(self, "model_options", {})
            current.update(preserved_options)
            self.model_options = current

        # 4. æ›´æ”¹ç±»æŒ‡é’ˆ
        try: self.__class__ = real_obj.__class__
        except: pass
        if hasattr(self, "_laoli_is_shadow"): del self._laoli_is_shadow
    
    def copy(self): return self.clone()
    def clone(self, *args, **kwargs):
        if self._laoli_real_obj: return self._laoli_real_obj.clone()
        # åˆ›å»ºæ–°çš„å½±å­å‰¯æœ¬
        new_shadow = ShadowPatcher(self._laoli_group)
        new_shadow.patches = copy.deepcopy(getattr(self, "patches", {}))
        new_shadow.object_patches = copy.deepcopy(getattr(self, "object_patches", {}))
        new_shadow.model_options = copy.deepcopy(getattr(self, "model_options", {}))
        return new_shadow

    def __getattr__(self, name):
        if name.startswith("_laoli"): raise AttributeError(name)
        real = self.__dict__.get("_laoli_real_obj", None)
        # å¤„ç† pinned ç­‰ç‰¹æ®Šå±æ€§
        if name == "pinned" and real is None: return set()
        if real is None: real = self._ensure_real()
        return getattr(real, name)

class ShadowGenericProxy:
    """é€šç”¨çš„å½±å­ä»£ç†ï¼ˆç”¨äº CLIP, VAEï¼‰"""
    def __init__(self, group, index):
        self._laoli_group = group
        self._laoli_index = index
        self._laoli_real_obj = None
    
    def _ensure_real(self):
        if self._laoli_real_obj is None:
            real = self._laoli_group.get_real_thing(self._laoli_index)
            self.become_real(real)
        return self._laoli_real_obj

    def become_real(self, real_obj):
        self._laoli_real_obj = real_obj
        self.__dict__.update(real_obj.__dict__)
        try: self.__class__ = real_obj.__class__
        except: pass

    def clone(self):
        if self._laoli_real_obj: return self._laoli_real_obj.clone()
        return ShadowGenericProxy(self._laoli_group, self._laoli_index)

    # é€ä¼ å¸¸è§æ–¹æ³•
    def decode(self, samples_in): return self._ensure_real().decode(samples_in)
    def encode(self, pixel_samples): return self._ensure_real().encode(pixel_samples)

    def __getattr__(self, name):
        if name.startswith("_laoli"): raise AttributeError(name)
        real = self._ensure_real()
        return getattr(real, name)

# ==========================================================
# 3. åŠ«æŒåŠ è½½å™¨ (Hooks)
# ==========================================================

# --- Checkpoint Loader åŠ«æŒ ---
if not hasattr(comfy.sd, "_laoli_org_load_ckpt"):
    comfy.sd._laoli_org_load_ckpt = comfy.sd.load_checkpoint_guess_config

def _hacked_load_checkpoint(*args, **kwargs):
    # ä½¿ç”¨ *args å…¼å®¹æ‰€æœ‰ç‰ˆæœ¬
    if Shadow_Config.enabled and Shadow_Config.shadow_mode:
        # å°è¯•è·å–æ–‡ä»¶åç”¨äºæ—¥å¿—
        try:
            ckpt_path = args[0] if len(args) > 0 else kwargs.get("ckpt_path", "Unknown")
            name = os.path.basename(ckpt_path)
        except: name = "Unknown"

        if Shadow_Config.verbose: print(f"ğŸ’¤ [LaoLi Shadow] æ‹¦æˆª Checkpoint: {name}")
        
        group = ShadowGroup(name, comfy.sd._laoli_org_load_ckpt, *args, **kwargs)
        
        # è¿”å›å½±å­ä¸‰å‰‘å®¢ (Model, CLIP, VAE)
        # æ³¨æ„ï¼šå¦‚æœåŸå§‹å‡½æ•°è¿”å›4ä¸ªå€¼(clipvision)ï¼Œè¿™é‡Œåªè¿”å›å‰3ä¸ªå½±å­ï¼Œç¬¬4ä¸ªä¼šä¸¢å¤±ã€‚
        # ä½†é€šå¸¸ CheckpointLoaderSimple åªè§£åŒ…å‰3ä¸ªã€‚
        return (ShadowPatcher(group), ShadowGenericProxy(group, 1), ShadowGenericProxy(group, 2))
        
    return comfy.sd._laoli_org_load_ckpt(*args, **kwargs)

if hasattr(comfy.sd, "load_checkpoint_guess_config"):
    comfy.sd.load_checkpoint_guess_config = _hacked_load_checkpoint

# --- ControlNet Loader åŠ«æŒ ---
class ShadowControlNet(ModelPatcher):
    def __init__(self, name, loader):
        dummy = torch.nn.Module()
        super().__init__(dummy, torch.device("cpu"), torch.device("cpu"))
        self._laoli_is_shadow = True
        self._laoli_name = name
        self._laoli_loader = loader
        self._laoli_real = None
    
    def summon(self):
        if self._laoli_real: return self._laoli_real
        if Shadow_Config.verbose: print(f"ğŸ‘» [LaoLi Shadow] å”¤é†’ ControlNet: {self._laoli_name}")
        self._laoli_real = self._laoli_loader()
        self.__dict__.update(self._laoli_real.__dict__)
        try: self.__class__ = self._laoli_real.__class__
        except: pass
        if hasattr(self, "_laoli_is_shadow"): del self._laoli_is_shadow
        return self._laoli_real

    def copy(self):
        if self._laoli_real: return self._laoli_real.copy()
        return ShadowControlNet(self._laoli_name, self._laoli_loader)
    
    def __getattr__(self, name):
        if name.startswith("_laoli"): raise AttributeError(name)
        real = self.__dict__.get("_laoli_real", None)
        if name == "pinned" and real is None: return set()
        self.summon()
        return getattr(self._laoli_real, name)

if not hasattr(comfy.controlnet, "_laoli_org_load_cn"):
    comfy.controlnet._laoli_org_load_cn = comfy.controlnet.load_controlnet

def _hacked_load_controlnet(*args, **kwargs):
    if Shadow_Config.enabled and Shadow_Config.shadow_mode:
        try:
            ckpt_path = args[0] if len(args) > 0 else kwargs.get("ckpt_path", "Unknown")
            name = os.path.basename(ckpt_path)
        except: name = "Unknown"
        
        if Shadow_Config.verbose: print(f"ğŸ’¤ [LaoLi Shadow] æ‹¦æˆª ControlNet: {name}")
        return ShadowControlNet(name, lambda: comfy.controlnet._laoli_org_load_cn(*args, **kwargs))
    
    return comfy.controlnet._laoli_org_load_cn(*args, **kwargs)

if hasattr(comfy.controlnet, "load_controlnet"):
    comfy.controlnet.load_controlnet = _hacked_load_controlnet

# ==========================================================
# 4. æ˜¾å­˜ç®¡ç† (The Brain)
# ==========================================================
if not hasattr(mm, "_laoli_original_load_models_gpu"):
    mm._laoli_original_load_models_gpu = mm.load_models_gpu

def _shadow_load_models_gpu(models, memory_required=0, **kwargs):
    if Shadow_Config.enabled:
        try:
            device = mm.get_torch_device()
            # 1. å”¤é†’æ‰€æœ‰æ¶‰åŠçš„å½±å­
            for model in models:
                if getattr(model, "_laoli_is_shadow", False):
                    if hasattr(model, "summon"): model.summon() 
                    if hasattr(model, "_ensure_real"): model._ensure_real()
            
            # 2. æ˜¾å­˜ç­–ç•¥
            if Shadow_Config.mode == "Ease Mode" and device.type == 'cuda':
                mm.soft_empty_cache()
                
                # å¦‚æœæ˜¯å›¾ç‰‡æ¨¡å¼ï¼Œä¸”æ˜¾å­˜è¦æ±‚è¾ƒé«˜ï¼Œæ‰§è¡Œæ›´æ¿€è¿›çš„æ£€æŸ¥
                if "Image" in Shadow_Config.job_type:
                    try:
                        stats = torch.cuda.get_device_properties(device)
                        total_mem = stats.total_memory
                        reserved = torch.cuda.memory_reserved(device)
                        free_mem = total_mem - reserved
                        
                        needed = memory_required if memory_required > 0 else (1.5 * 1024**3)
                        reserve_bytes = Shadow_Config.vram_reserve_mb * 1024 * 1024
                        
                        # ç­–ç•¥ï¼šå¦‚æœæ€»æ˜¾å­˜ < 12GB (å°æ˜¾å­˜å¡) æˆ–è€… å‰©ä½™ç©ºé—´ä¸¥é‡ä¸è¶³
                        is_low_vram_card = total_mem < (12 * 1024**3)
                        is_critical = free_mem < (needed + reserve_bytes)
                        
                        if is_critical:
                            if is_low_vram_card:
                                if Shadow_Config.verbose: print("ğŸ§¹ [LaoLi Shadow] å°æ˜¾å­˜ä¿æŠ¤ -> æ·±åº¦æ¸…ç† (Unload All)")
                                mm.unload_all_models()
                                mm.soft_empty_cache()
                                torch.cuda.empty_cache()
                            else:
                                if Shadow_Config.verbose: print("ğŸ§¹ [LaoLi Shadow] ç©ºé—´å ç”¨æ¯”ä¾‹é«˜ -> æ™ºèƒ½è…¾æŒª")
                                mm.free_memory(needed + reserve_bytes, device)
                                mm.soft_empty_cache()
                    except: pass

        except Exception as e: 
            print(f"âŒ [LaoLi Shadow Error] æ˜¾å­˜ç®¡ç†å¼‚å¸¸: {e}")
    
    return mm._laoli_original_load_models_gpu(models, memory_required=memory_required, **kwargs)

mm.load_models_gpu = _shadow_load_models_gpu

# ==========================================================
# 5. èŠ‚ç‚¹å®šä¹‰ (Nodes)
# ==========================================================

# --- èŠ‚ç‚¹ 1: å…¨å±€é…ç½® ---
class LaoLi_Shadow_Node:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "enable": ("BOOLEAN", {"default": True}), 
                "job_type": (["Image (Aggressive)", "Video (Safe)"], {"default": "Image (Aggressive)"}), 
                "shadow_mode": ("BOOLEAN", {"default": True}),
                "mode": (["Ease Mode", "Monitor Mode"],),
                "ram_reserve_gb": ("FLOAT", {"default": 4.0, "min": 0.5, "max": 64.0, "step": 0.5}),
                "vram_reserve_mb": ("FLOAT", {"default": 512.0, "min": 0.0, "max": 8192.0, "step": 64.0}),
                "verbose": ("BOOLEAN", {"default": True}),
            }
        }
    RETURN_TYPES = ()
    FUNCTION = "update_settings"
    CATEGORY = "LaoLi Shadow"
    DESCRIPTION = "å½±å­ç³»ç»Ÿæ§åˆ¶å° - å¿…é¡»è¿æ¥åœ¨å·¥ä½œæµä¸­"
    OUTPUT_NODE = True # æ ‡è®°ä¸ºè¾“å‡ºèŠ‚ç‚¹

    @classmethod
    def IS_CHANGED(cls, **kwargs):
        return float("nan") # å¼ºåˆ¶æ¯æ¬¡è¿è¡Œ

    def update_settings(self, enable, job_type, shadow_mode, mode, ram_reserve_gb, vram_reserve_mb, verbose):
        Shadow_Config.enabled = enable
        Shadow_Config.job_type = job_type
        Shadow_Config.shadow_mode = shadow_mode
        Shadow_Config.mode = mode
        Shadow_Config.ram_reserve_gb = float(ram_reserve_gb)
        Shadow_Config.vram_reserve_mb = float(vram_reserve_mb)
        Shadow_Config.verbose = verbose
        
        if verbose:
            icon = "ğŸ–¼ï¸" if "Image" in job_type else "ğŸ¥"
            status = "ğŸŸ¢ ON" if enable else "ğŸ”´ OFF"
            print(f"ğŸ‘» [LaoLi Shadow] {status} | {icon} {job_type}")
            
        return ()

# --- èŠ‚ç‚¹ 2: é€»è¾‘é—¨ ---
class LaoLi_Flow_Gate:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": { "input_data": (any_type,) },
            "optional": { "wait_for": (any_type,) }
        }
    RETURN_TYPES = (any_type,) 
    FUNCTION = "run"
    CATEGORY = "LaoLi Shadow"
    DESCRIPTION = "æµç¨‹æ§åˆ¶ï¼šç­‰å¾… wait_for å®Œæˆåæ‰è¾“å‡º input_data"
    
    def run(self, input_data, wait_for=None): return (input_data,)

# --- èŠ‚ç‚¹ 3: æ˜¾å­˜æ’é˜Ÿä¼˜åŒ– ---
class LaoLi_Lineup_Node:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": (any_type,),
                "vram_threshold": ("FLOAT", {"default": 0.85, "min": 0.1, "max": 1.0, "step": 0.05}),
                "cleaning_interval": ("INT", {"default": 1, "min": 1, "max": 20, "step": 1}),
                "strict_mode": ("BOOLEAN", {"default": true}),
            }
        }
    RETURN_TYPES = (any_type,)
    RETURN_NAMES = ("optimized_model",)
    FUNCTION = "apply_lineup"
    CATEGORY = "LaoLi Shadow" 
    DESCRIPTION = "Lineup: åœ¨æ¨¡å‹å±‚é—´æ’å…¥æ˜¾å­˜æ£€æŸ¥ç‚¹"

    @classmethod
    def IS_CHANGED(cls, **kwargs):
        return float("nan") # å¼ºåˆ¶æ¯æ¬¡è¿è¡Œ

    def apply_lineup(self, model, vram_threshold, cleaning_interval, strict_mode):
        if not Shadow_Config.enabled: return (model,)

        target_model_wrapper = model
        try:
            # 1. è§£å¼€å½±å­
            if getattr(model, "_laoli_is_shadow", False):
                if Shadow_Config.verbose: print(f"âš¡ [LaoLi Lineup] å”¤é†’å½±å­æ¨¡å‹ä»¥æ³¨å…¥é’©å­...")
                if hasattr(model, "_ensure_real"):
                    model._ensure_real()
                    target_model_wrapper = model._laoli_real_obj
            elif hasattr(model, "clone"):
                try: target_model_wrapper = model.clone()
                except: target_model_wrapper = model
            
            # 2. å‡†å¤‡æ•°æ®
            device = mm.get_torch_device()
            total_vram = 0
            if device.type == 'cuda':
                total_vram = torch.cuda.get_device_properties(device).total_memory
            
            reserve_bytes = Shadow_Config.vram_reserve_mb * 1024 * 1024
            limit_bytes = min(total_vram * vram_threshold, total_vram - reserve_bytes)
            
            # 3. é’©å­é€»è¾‘
            def smart_hook(module, input):
                if total_vram == 0: return None
                try:
                    if torch.cuda.memory_reserved(device) >= limit_bytes:
                        if strict_mode and "Video" not in Shadow_Config.job_type: 
                            torch.cuda.synchronize()
                        mm.soft_empty_cache()
                except: pass
                return None

            # 4. å¯»æ‰¾å±‚ç»“æ„
            best_container = self._find_dominant_layer_container(target_model_wrapper)
            if best_container:
                blocks = list(best_container)
                count = 0
                for i, block in enumerate(blocks):
                    if i % cleaning_interval == 0 and hasattr(block, "register_forward_pre_hook"):
                        block.register_forward_pre_hook(smart_hook)
                        count += 1
                if Shadow_Config.verbose:
                    print(f"ğŸš€ [LaoLi Lineup] ä¼˜åŒ–å®Œæ¯•: æŒ‚è½½ {count} ä¸ªæ¸…ç†å“¨å…µ")
            else:
                if Shadow_Config.verbose: print(f"âš ï¸ [LaoLi Lineup] æ— æ³•è¯†åˆ«æ¨¡å‹ç»“æ„ï¼Œè·³è¿‡ä¼˜åŒ–")

            return (target_model_wrapper,)

        except Exception as e:
            print(f"âŒ [LaoLi Lineup] ä¼˜åŒ–å¤±è´¥: {e}")
            return (model,)

    def _find_dominant_layer_container(self, root_obj):
        real_model = root_obj
        if hasattr(real_model, "model"): real_model = real_model.model
        if hasattr(real_model, "diffusion_model"): real_model = real_model.diffusion_model
        
        best_container = None
        max_len = 0
        try:
            for name, module in real_model.named_modules():
                if isinstance(module, (nn.ModuleList, nn.Sequential)):
                    if len(module) > max_len and len(module) > 4:
                        max_len = len(module)
                        best_container = module
        except: pass
        return best_container

# ==========================================================
# 6. Prompt é¢„æ‰«æ (ä¿æŒé…ç½®åŒæ­¥)
# ==========================================================
try:
    import server
    if not hasattr(server.PromptServer, "_laoli_original_trigger"):
        server.PromptServer._laoli_original_trigger = server.PromptServer.trigger_computation

    def _shadow_hooked_trigger(self, prompt, id, *args, **kwargs):
        # é»˜è®¤å…³é—­ï¼Œç­‰å¾…æ‰«ææ¿€æ´»
        Shadow_Config.enabled = False
        try:
            for uid, data in prompt.items():
                if data.get('class_type') == 'LaoLi_Shadow':
                    inputs = data.get('inputs', {})
                    Shadow_Config.enabled = inputs.get('enable', True)
                    Shadow_Config.job_type = inputs.get('job_type', "Image")
                    Shadow_Config.shadow_mode = inputs.get('shadow_mode', True)
                    Shadow_Config.mode = inputs.get('mode', "Ease Mode")
                    Shadow_Config.verbose = inputs.get('verbose', True)
                    # ç®€å•è¯»å–æ•°å€¼
                    try: Shadow_Config.vram_reserve_mb = float(inputs.get('vram_reserve_mb', 512.0))
                    except: pass
                    break
        except: pass
        return server.PromptServer._laoli_original_trigger(self, prompt, id, *args, **kwargs)

    server.PromptServer.trigger_computation = _shadow_hooked_trigger
except: pass

# ==========================================================
# 7. æ³¨å†ŒèŠ‚ç‚¹
# ==========================================================
NODE_CLASS_MAPPINGS = {
    "LaoLi_Shadow": LaoLi_Shadow_Node,
    "LaoLi_Flow_Gate": LaoLi_Flow_Gate,
    "LaoLi_Lineup": LaoLi_Lineup_Node
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "LaoLi_Shadow": "ğŸ‘» è€æ_å½±å­ (Shadow) ",
    "LaoLi_Flow_Gate": "ğŸš§ è€æ_é€»è¾‘é—¨ (Flow Gate)",
    "LaoLi_Lineup": "ğŸš€ è€æ_æ’é˜Ÿ (Lineup)"
}